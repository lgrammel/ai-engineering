# Distillation

Distillation is a [training](./training.md) technique where a smaller "student" model learns to replicate the behavior of a larger "teacher" model, typically by training on the teacher's outputs rather than (or in addition to) original labeled data.

The goal is to transfer the teacher's capabilities into a model that is cheaper and faster to run at [inference](./inference.md) time, at the cost of some capability loss. Distillation can target the teacher's output probabilities (soft labels), its intermediate representations, or simply its generated text ([synthetic data](./synthetic-data.md)). It is commonly used by [model developers](./model-developer.md) to create smaller variants of flagship models and by application developers to produce task-specific models from general-purpose ones.

Distillation sits between [pretraining](./pretraining.md) (broad learning from raw data) and task-specific [fine-tuning](./fine-tuning.md) (narrow adaptation): it produces a model that retains much of the teacher's general ability while being significantly more efficient. Combined with [model quantization](./model-quantization.md), distillation is a primary technique for making large models practical in resource-constrained deployment settings.

## Examples

- A model developer releasing a 8B-parameter distilled variant of a 70B-parameter flagship model.
- An application developer distilling a general-purpose model into a task-specific model that handles a narrow domain at lower cost.
- Training a student model on synthetic responses generated by a teacher model for a specific task.

## Synonyms

knowledge distillation, model distillation

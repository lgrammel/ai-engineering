# Transformer Architecture

The transformer architecture is a neural network design for sequence modeling built around attention (especially self-attention), enabling a model to relate [tokens](./token.md) to each other across a [context window](./context-size.md) while processing inputs in parallel. It is the architecture underlying virtually all current [LLMs](./llm.md) and [embedding models](./embedding-model.md), established during [pretraining](./pretraining.md).

## Synonyms

transformer, attention-based architecture

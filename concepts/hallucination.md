# Hallucination

A hallucination is output generated by an [LLM](./llm.md) that is fluent and plausible-sounding but factually incorrect or fabricated. Hallucinations arise because language models produce text based on statistical patterns rather than verified knowledge, so they can confidently state things that have no basis in their training data or the provided [context](./context.md).

In [agent](./agent.md) systems, hallucinations are particularly risky because they can propagate through multi-step workflows. A hallucinated fact can feed into [tool](./tools.md) calls (e.g., querying a database with a fabricated identifier or calling an API with incorrect parameters), and the resulting errors or misleading outputs can reinforce the original falsehood. In multi-agent pipelines, one agent's hallucinated output may be accepted as fact by downstream agents, compounding the error across the entire workflow.

Common contributing factors include ambiguous prompts, insufficient context, and queries that fall outside the model's training distribution.

## Examples

- A model invents a plausible-looking but nonexistent citation (author, title, journal).
- An agent hallucinates a package name, installs it via a tool call, and a typosquatted malicious package gets executed.
- One agent in a pipeline produces a fabricated statistic that downstream agents incorporate into reports without verification.

# Inference Provider

An organization that runs [LLMs](./llm.md) to generate outputs ([inference](./inference.md)) and exposes them via an API or hosted service. It may serve its own [LLMs](./llm.md) or third-party [LLMs](./llm.md).

Note: many organizations are both [model developers](./model-developer.md) and inference providers; these are roles, not mutually exclusive categories.

## Examples

- OpenAI
- Anthropic
- Google (Gemini API)
- AWS Bedrock
- Together AI
- Fireworks AI
